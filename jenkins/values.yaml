# For updating, delete pvc jenkins-docker-client
dockerClientVersion: 19.03.9

master:
  # to prevent the jenkins-ui-test pod being created
  testEnabled: false
  # Use deterministic version and not "lts", which is pretty much the same as "latest".
  # Note: When updating, check if PATH of image still matches the one listed in "containerEnv"
  tag: 2.249.3-lts-jdk11

  serviceType: NodePort
  nodePort: 9090

  additionalPlugins:
    - docker-workflow:1.25
    - docker-plugin:1.2.1
    - job-dsl:1.77
    - pipeline-utility-steps:2.6.1
    - junit:1.44
    - scm-manager:1.5.0

  # This would be great to not install every plugin again on startup.
  # BUT: This also leads to CASC being ignored.
  # initializeOnce: true

  # Allow builds on master in order to facilitate an agent with docker CLI
  numExecutors: 2

  JCasC:
    configScripts:
      # TODO use SCMM user secret keys for username and password
      scmm-credentials: |
        credentials:
          system:
            domainCredentials:
            - credentials:
              - usernamePassword:
                  id: "scmm-user"
                  username: "${USERNAME}"
                  password: "${PASSWORD}"
                  description: "Credentials for accessing SCM-Manager"
                  scope: GLOBAL
      # Set some more details on master agent
      # Add /docker to the PATH. Using "PATH=:/docker:${PATH}" is not working here, so copy PATH from container :-/
      # That is, this might have to be updated with newer version of the image
      master-agent: |
        jenkins:
          labelString: "docker"
          nodeProperties:
            - envVars:
                env:
                - key: "PATH"
                  value: "/opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/docker"
      # TODO could we use the SCMM source here?
      # These URLS could be helpful:
      # http://localhost:9090/job/petclinic-plain/config.xml
      # http://localhost:9090//plugin/job-dsl/api-viewer/index.html#path/multibranchPipelineJob-branchSources
      # For now using "scm-manager" in JobDSL leads to an error when CASC is applied: 
      # "No such property: scm for class: javaposse.jobdsl.plugin.structs.DescribableListContext"
      # This is probably because "scm-manager" is invalid groovy syntax.
      # See https://github.com/jenkinsci/scm-manager-plugin/blob/1.4.0/src/main/java/com/cloudogu/scmmanager/scm/ScmManagerSource.java
      # This might be fixed in a later SCMM Plugin for Jenkins
      init-job: |
        jenkins:
          systemMessage: "Seeding init jobs"
        jobs:
          - script: |
              multibranchPipelineJob('fluxV1-petclinic-plain') {
                branchSources {
                  git {
                    id('fluxV1-petclinic-plain')
                    remote('http://scmm-scm-manager:9091/scm/repo/application/petclinic-plain')
                    credentialsId('scmm-user')
                  }
                }
              }

          - script: |
              multibranchPipelineJob('nginx') {
                  branchSources {
                      git {
                          id('nginx')
                          remote('http://scmm-scm-manager:9091/scm/repo/application/nginx')
                          credentialsId('scmm-user')
                      }
                  }
              }
          - script: |
              multibranchPipelineJob('fluxv2-petclinic-plain') {
                  branchSources {
                      git {
                          id('fluxv2-petclinic-plain')
                          remote('http://scmm-scm-manager:9091/scm/repo/fluxv2/petclinic-plain')
                          credentialsId('scmm-user')
                      }
                  }
              }
          - script: |
              multibranchPipelineJob('argocd-petclinic-plain') {
                  branchSources {
                      git {
                          id('argocd-petclinic-plain')
                          remote('http://scmm-scm-manager:9091/scm/repo/argocd/petclinic-plain')
                          credentialsId('scmm-user')
                      }
                  }
              }

  sidecars:
    configAutoReload:
      enabled: false

  admin:
    # Use reproducible admin password from secret. Change there, if necessary.
    existingSecret: jenkins-credentials

  containerEnv:
    - name: SECRETS
      # The files in this folders can be used as ${variable} in CasC credentials
      # Default /run/secrets results in "read-only file system"
      value: /secrets/jenkins
    - name: PATH
      # We already mounted this PATH on the master-agent. Still, "docker.inside {}" failes in pipeline?
      # Why? The docker pipeline plugin seems to set an empty environment: https://github.com/jenkinsci/docker-workflow-plugin/blob/docker-workflow-1.25/src/main/java/org/jenkinsci/plugins/docker/workflow/client/DockerClient.java#L261
      # Workaround: Set the ENV in the container:
      value: /opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/docker

  customInitContainers:
    # Download docker client
    - name: download-docker-client
      image: alpine:3.12.0
      imagePullPolicy: "{{ .Values.master.imagePullPolicy }}"
      command: ["/bin/sh", "-c"]
      args:
        - set -x;
          if [ -f /docker/docker ]; then echo 'Docker already installed'; exit 0; fi;
          cd /tmp;
          wget -qc https://download.docker.com/linux/static/stable/x86_64/docker-{{.Values.dockerClientVersion}}.tgz -O - | tar -xz;
          mv /tmp/docker/docker /docker/docker;
      volumeMounts:
        - name: docker-client
          mountPath: /docker

  # Allow access to docker socket. Running as root is not a good practice, but seems pragmatic here.
  # Because other option setting "fsgroup" depends on the group ID of the "docker" group on each host.
  # This would require individual configuration,
  # Calling: "getent group docker | awk -F: '{ print $3}'" and setting the result manually as fsgroup.
  #  fsGroup: 998
  # Also, with access to the docker socket, the container has root access to the host anyway. So why bother about the container?
  # It's a demo, after all!
  runAsUser: 0

persistence:
  volumes:
    - name: docker-client
      # Persist, so the client does not have to be downloaded on each start
      persistentVolumeClaim:
        claimName: jenkins-docker-client
    - name: docker-sock # For this demo, allow jenkins master to access docker client
      hostPath:
        path: /var/run/docker.sock
    # In our local playground infrastructure, we're using a workaround: Builds are run on the Jenkins master, which runs
    # inside a container. During the builds, more containers are started (on the same docker host).
    # This leads to a scenario where the Jenkins container tries to mount its filesystem into another container.
    # The docker host is only able to realize this mounts when the mounted paths are the same inside and outside the containers.
    # As another workaround, we provide the path inside the container also outside the container (/var/jenkins_home/workspace)
    # In order not to pollute the host, we symlink /var/jenkins_home/workspace into this repo.
    - name: workspace
      hostPath:
        path: /var/jenkins_home/workspace
    - name: scmm-user
      secret:
        # TODO use SCMM user secret here
        secretName: jenkins-scmm

  mounts:
    - name: workspace
      mountPath: "/var/jenkins_home/workspace"
    - name: scmm-user
      # Use k8s secret as jenkins credentials.
      # https://github.com/jenkinsci/configuration-as-code-plugin/blob/master/docs/features/secrets.adoc#kubernetes-secrets
      mountPath: /secrets/jenkins
      readOnly: true
    - name: docker-client
      mountPath: /docker
      readOnly: true
    - name: docker-sock
      mountPath: "/var/run/docker.sock"
      readOnly: true