# For updating, delete pvc jenkins-docker-client
dockerClientVersion: 19.03.9

master:
  # to prevent the jenkins-ui-test pod being created
  testEnabled: false
  # Use deterministic version and not "lts", which is pretty much the same as "latest".
  # Note: When updating, check if PATH of image still matches the one listed in "containerEnv"
  tag: 2.249.2-lts-alpine

  serviceType: LoadBalancer
  servicePort: 80
  # Is ignored when type is LoadBalancer. For local cluster we change the service type to NodePort
  nodePort: 9090

  # OverwritePlugins: true

  # installPlugins:
  #   - credentials-binding:1.24
  #   - pipeline-job:2.40

  additionalPlugins:
    - docker-workflow:1.24
    - docker-plugin:1.2.1
    - job-dsl:1.77
    - pipeline-utility-steps:2.6.1
    - junit:1.34
    - scm-manager:1.4.0
    - html5-notifier-plugin:1.5

  # This would be great to not install every plugin again on startup.
  # BUT: This also leads to CASC being ignored.
  # initializeOnce: true

  # Don't use master for builds
  numExecutors: 0

  # Master and agents need to run on the same host. See comment above agent.workingDir for details.
  nodeSelector:
    node: jenkins

  JCasC:
    configScripts:
      # TODO use SCMM user secret keys for username and password
      scmm-credentials: |
        credentials:
          system:
            domainCredentials:
            - credentials:
              - usernamePassword:
                  id: "scmm-user"
                  username: "${USERNAME}"
                  password: "${PASSWORD}"
                  description: "Credentials for accessing SCM-Manager"
                  scope: GLOBAL

      # TODO could we use the SCMM source here?
      # These URLS could be helpful:
      # http://localhost:9090/job/petclinic-plain/config.xml
      # http://localhost:9090//plugin/job-dsl/api-viewer/index.html#path/multibranchPipelineJob-branchSources
      # For now using "scm-manager" in JobDSL leads to an error when CASC is applied: 
      # "No such property: scm for class: javaposse.jobdsl.plugin.structs.DescribableListContext"
      # This is probably because "scm-manager" is invalid groovy syntax.
      # See https://github.com/jenkinsci/scm-manager-plugin/blob/1.4.0/src/main/java/com/cloudogu/scmmanager/scm/ScmManagerSource.java
      # This might be fixed in a later SCMM Plugin for Jenkins
      init-job: |
        jenkins:
          systemMessage: "Seeding init jobs"
        jobs:
          - script: |
              multibranchPipelineJob('fluxv1-petclinic-plain') {
                  branchSources {
                      git {
                          id('fluxv1-petclinic-plain')
                          remote('http://scmm-scm-manager/scm/repo/fluxv1/petclinic-plain')
                          credentialsId('scmm-user')
                      }
                  }
              }
          - script: |
              multibranchPipelineJob('fluxv1-petclinic-helm') {
                branchSources {
                  git {
                    id('fluxv1-petclinic-helm')
                    remote('http://scmm-scm-manager/scm/repo/fluxv1/petclinic-helm')
                    credentialsId('scmm-user')
                  }
                }
              }
          - script: |
              multibranchPipelineJob('fluxv1-nginx') {
                  branchSources {
                      git {
                          id('fluxv1-nginx')
                          remote('http://scmm-scm-manager/scm/repo/fluxv1/nginx-helm')
                          credentialsId('scmm-user')
                      }
                  }
              }
          - script: |
              multibranchPipelineJob('fluxv2-petclinic-plain') {
                  branchSources {
                      git {
                          id('fluxv2-petclinic-plain')
                          remote('http://scmm-scm-manager/scm/repo/fluxv2/petclinic-plain')
                          credentialsId('scmm-user')
                      }
                  }
              }
          - script: |
              multibranchPipelineJob('argocd-petclinic-plain') {
                  branchSources {
                      git {
                          id('argocd-petclinic-plain')
                          remote('http://scmm-scm-manager/scm/repo/argocd/petclinic-plain')
                          credentialsId('scmm-user')
                      }
                  }
              }

  sidecars:
    configAutoReload:
      enabled: false

  admin:
    # Use reproducible admin password from secret. Change there, if necessary.
    existingSecret: jenkins-credentials

  containerEnv:
    - name: SECRETS
      # The files in this folders can be used as ${variable} in CasC credentials
      # Default /run/secrets results in "read-only file system"
      value: /secrets/jenkins
    - name: PATH
      # We already mounted this PATH on the master-agent. Still, "docker.inside {}" fails in pipeline?
      # Why? The docker pipeline plugin seems to set an empty environment: https://github.com/jenkinsci/docker-workflow-plugin/blob/docker-workflow-1.25/src/main/java/org/jenkinsci/plugins/docker/workflow/client/DockerClient.java#L261
      # Workaround: Set the ENV in the container:
      value: /opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tmp/docker
      
  customInitContainers:
    # Create Jenkins agent working dir explicitly. Otherwise it seems to be owned by root
    # The does not need this folder, but creating it in a defined way and ownership on startup is better than to run an
    # InitContainer on each agent startup
    - name: create-agent-working-dir
      securityContext:
        runAsUser: 1000
      image: bash:5.1.4
      imagePullPolicy: "{{ .Values.master.imagePullPolicy }}"
      command: [ "/usr/local/bin/bash", "-c" ]
      args:
        - set -x -o nounset -o pipefail -o errexit;
          id;
          if [[ ! -d /host-tmp/k8s-gitops-playground-jenkins-agent ]]; then
            echo creating /tmp/k8s-gitops-playground-jenkins-agent on host and chowning to UID 1000; 
            mkdir /host-tmp/k8s-gitops-playground-jenkins-agent;
          fi;
          
          if [[ -f /host-tmp/docker/docker ]]; then echo 'Docker already installed'; exit 0; fi;
          cd /host-tmp;
          wget -q https://download.docker.com/linux/static/stable/x86_64/docker-{{.Values.dockerClientVersion}}.tgz -O docker.tgz;
          tar -xzf docker.tgz;
          rm docker.tgz;
          find docker -type f -not -name 'docker' -delete;
          # Delete containerd, etc. We only need the docker CLI
          # Note: "wget -O- | tar" leads to the folder being owned by root, even when creating it beforehand?!
          # That's
      volumeMounts:
        - name: host-tmp
          mountPath: /host-tmp

persistence:
  volumes:
    - name: scmm-user
      secret:
        secretName: gitops-scmm
    # Needed for initContainer only
    - name: host-tmp
      hostPath:
        path: /tmp

  mounts:
    - name: scmm-user
      # Use k8s secret as jenkins credentials.
      # https://github.com/jenkinsci/configuration-as-code-plugin/blob/master/docs/features/secrets.adoc#kubernetes-secrets
      mountPath: /secrets/jenkins
      readOnly: true

agent:
  # We need JDK11 for our PetClinic example
  #tag: "4.3-4-jdk11"
  tag: "4.6-1-jdk11"
  # In our local playground infrastructure builds are run in agent containers (pods). During the builds, more
  # containers are started via the Jenkins Docker Plugin (on the same docker host).
  # This leads to a scenario where the agent container tries to mount its filesystem into another container.
  # The docker host is only able to realize this mounts when the mounted paths are the same inside and outside the
  # containers.
  # So as a workaround, we provide the path inside the container also outside the container.
  # The /tmp folder is a good choice, because it is writable for all users on the host.
  # One disadvantage is, that /tmp is deleted when the host shuts down. Which might slow down builds
  # A different option would be to link the workspace into this repo.
  # If we should ever want to implement this, the logic can be reused from Git History:
  # https://github.com/cloudogu/k8s-gitops-playground/blob/61e033/scripts/apply.sh#L211-L235
  # We mount the same PATH as a hostPath. See bellow.
  # On Multi Node Clusters this leads to the requirement that Jenkins Master and agents run on the same host
  # We realize this using nodeSelectors 
  workingDir: "/tmp/k8s-gitops-playground-jenkins-agent"
  nodeSelector:
    node: jenkins
  # Number of concurrent builds. Keep it low to avoid high CPU load.
  containerCap: 1
  customJenkinsLabels: [ 'docker' ]
  resources:
    limits:
      cpu: "1"
      memory: "1Gi"
  envVars:
    - name: PATH
      # Add /tmp/docker to the path
      value: /usr/local/openjdk-11/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tmp/docker
  volumes:
    - type: HostPath
      # See workingDir
      hostPath: /tmp/k8s-gitops-playground-jenkins-agent
      mountPath: /tmp/k8s-gitops-playground-jenkins-agent
    - type: HostPath
      # For this demo, allow jenkins master to access docker client
      hostPath: /var/run/docker.sock
      mountPath: /var/run/docker.sock
    - type: HostPath
      # Use a static docker binary.
      # It's downloaded by the jenkins master's init container, so it needs to be done only once.
      hostPath: /tmp/docker/
      mountPath: /tmp/docker/
  # Controls how agent pods are retained after the Jenkins build completes
  # Not in Jenkins but in K8s. Helpful for debugging
  # In order to get rid of those many "default-xyz" pod use: kubectl delete pod -l jenkins/jenkins-jenkins-slave=true
  podRetention: "OnFailure"

